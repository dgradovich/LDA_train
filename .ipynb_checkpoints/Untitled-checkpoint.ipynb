{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Documents\n",
    "documents = ['eat turkey on turkey day holiday',\n",
    "          'i like to eat cake on holiday',\n",
    "          'turkey trot race on thanksgiving holiday',\n",
    "          'snail race the turtle',\n",
    "          'time travel space race',\n",
    "          'movie on thanksgiving',\n",
    "          'movie at air and space museum is cool movie',\n",
    "          'aspiring movie star']\n",
    "\n",
    "documents = [doc.split(' ') for doc in documents]\n",
    "## Create Vocabulary\n",
    "vocabulary = reduce(operator.concat, documents)\n",
    "vocabulary = reduce(lambda l, x: l+[x] if x not in l else l, vocabulary, [])\n",
    "## Id Assigned Vocabulary\n",
    "vocabulary = OrderedDict([(vocabulary[i],i) for i in range(len(vocabulary))])\n",
    "## Id Assigned Document\n",
    "def assign_ids(document, vocabulary):\n",
    "    return [vocabulary[word] for word in document]\n",
    "\n",
    "documents_with_ids = [assign_ids(doc, vocabulary) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "K = 2 # number of topics\n",
    "alpha = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "eta = .001 # hyperparameter for Posson Sampling\n",
    "iterations = 3 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Randomly assign topics to words in each doc. \n",
    "## 2. Generate word-topic count matrix.\n",
    "word_topic = np.zeros([K, len(vocabulary)])  # initialize word-topic count matrix\n",
    "topic_assignment = np.array([np.zeros(len(doc)) for doc in documents_with_ids]) # initialize topic assignment list\n",
    "for d in range(len(documents_with_ids)): # for each document\n",
    "    for w in range(len(documents_with_ids[d])): # for each token in document d\n",
    "        topic_assignment[d][w] = np.random.randint(0,K,1) # randomly assign topic to token w.\n",
    "        ti = topic_assignment[d][w] # topic index\n",
    "        wi = documents_with_ids[d][w] # wordID for token w\n",
    "        word_topic[np.int(ti),wi] +=1 # update word-topic count matrix     \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Document-topic matrix\n",
    "\n",
    "document_topic = np.zeros([len(documents_with_ids), K])\n",
    "for d in range(len(documents_with_ids)): # for each document d\n",
    "    for t in range(K): # for each topic t\n",
    "        document_topic[d,t] = np.sum(topic_assignment[d] == t) # count tokens in document d assigned to topic t   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 0  token: 1  topic: 0 => 1\n",
      "doc: 0  token: 2  topic: 0 => 1\n",
      "doc: 0  token: 3  topic: 1 => 0\n",
      "doc: 0  token: 5  topic: 1 => 0\n",
      "doc: 1  token: 0  topic: 0 => 1\n",
      "doc: 1  token: 1  topic: 0 => 1\n",
      "doc: 2  token: 0  topic: 0 => 1\n",
      "doc: 2  token: 1  topic: 1 => 0\n",
      "doc: 2  token: 2  topic: 1 => 0\n",
      "doc: 2  token: 4  topic: 0 => 1\n",
      "doc: 3  token: 1  topic: 0 => 1\n",
      "doc: 3  token: 2  topic: 1 => 0\n",
      "doc: 4  token: 0  topic: 0 => 1\n",
      "doc: 4  token: 3  topic: 1 => 0\n",
      "doc: 5  token: 0  topic: 1 => 0\n",
      "doc: 6  token: 1  topic: 0 => 1\n",
      "doc: 6  token: 3  topic: 1 => 0\n",
      "doc: 6  token: 5  topic: 0 => 1\n",
      "doc: 6  token: 8  topic: 1 => 0\n",
      "doc: 0  token: 1  topic: 1 => 0\n",
      "doc: 1  token: 1  topic: 1 => 0\n",
      "doc: 1  token: 2  topic: 1 => 0\n",
      "doc: 2  token: 0  topic: 1 => 0\n",
      "doc: 3  token: 1  topic: 1 => 0\n",
      "doc: 3  token: 2  topic: 0 => 1\n",
      "doc: 3  token: 3  topic: 0 => 1\n",
      "doc: 4  token: 1  topic: 1 => 0\n",
      "doc: 5  token: 0  topic: 0 => 1\n",
      "doc: 6  token: 0  topic: 1 => 0\n",
      "doc: 6  token: 1  topic: 1 => 0\n",
      "doc: 6  token: 2  topic: 1 => 0\n",
      "doc: 6  token: 3  topic: 0 => 1\n",
      "doc: 6  token: 5  topic: 1 => 0\n",
      "doc: 6  token: 6  topic: 0 => 1\n",
      "doc: 6  token: 7  topic: 1 => 0\n",
      "doc: 7  token: 2  topic: 0 => 1\n",
      "doc: 1  token: 0  topic: 1 => 0\n",
      "doc: 1  token: 2  topic: 0 => 1\n",
      "doc: 5  token: 0  topic: 1 => 0\n",
      "doc: 6  token: 1  topic: 0 => 1\n",
      "doc: 6  token: 2  topic: 0 => 1\n",
      "doc: 6  token: 6  topic: 1 => 0\n",
      "doc: 7  token: 2  topic: 1 => 0\n"
     ]
    }
   ],
   "source": [
    "# Gibbs sampling\n",
    "for i in range(iterations): # for each pass through the corpus\n",
    "    for d in range(len(documents_with_ids)): # for each document\n",
    "        for w in range(len(documents_with_ids[d])): # for each token \n",
    "      \n",
    "            t0 = np.int(topic_assignment[d][w]) # initial topic assignment to token w\n",
    "            wid = np.int(documents_with_ids[d][w]) # wordID of token w\n",
    "            document_topic[d,t0] -= 1 # we don't want to include token w in our document-topic count matrix when sampling for token w\n",
    "            word_topic[t0,wid] -= 1 # we don't want to include token w in our word-topic count matrix when sampling for token w\n",
    "\n",
    "            ## UPDATE TOPIC ASSIGNMENT FOR EACH WORD -- COLLAPSED GIBBS SAMPLING MAGIC.  Where the magic happens.\n",
    "            denom_a = np.sum(document_topic[d,:]) + K * alpha # number of tokens in document + number topics * alpha\n",
    "            denom_b = np.sum(word_topic, axis=1) + len(vocabulary) * eta # number of tokens in each topic + # of words in vocab * eta\n",
    "            p_z = (word_topic[:,wid] + eta)/denom_b*(document_topic[d,:] + alpha)/denom_a # calculating probability word belongs to each topic\n",
    "            t1 = np.random.choice(list(range(K)), p=p_z/sum(p_z)) # draw topic for word n from multinomial using probabilities calculated above\n",
    "\n",
    "            topic_assignment[d][w] = t1 # update topic assignment list with newly sampled topic for token w.\n",
    "            document_topic[d,t1] += 1 # re-increment document-topic matrix with new topic assignment for token w.\n",
    "            word_topic[t1,wid] += 1 #re-increment word-topic matrix with new topic assignment for token w.\n",
    "\n",
    "            if t0!=t1:\n",
    "                print('doc:', d, ' token:' ,w, ' topic:',t0,'=>',t1) # examine when topic assignments change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.625, 0.375]), array([0.55555556, 0.44444444]), array([0.625, 0.375]), array([0.33333333, 0.66666667]), array([0.66666667, 0.33333333]), array([0.4, 0.6]), array([0.63636364, 0.36363636]), array([0.8, 0.2])]\n"
     ]
    }
   ],
   "source": [
    "theta = [(document_topic+alpha)[i] / np.sum(document_topic+alpha, axis=1)[i] for i in range(len(documents_with_ids))] # topic probabilities per document\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([3.84216391e-05, 1.15303339e-01, 3.84216391e-05, 3.84600607e-02,\n",
      "       1.15303339e-01, 3.84600607e-02, 3.84600607e-02, 3.84216391e-05,\n",
      "       3.84600607e-02, 3.84600607e-02, 1.15303339e-01, 3.84216391e-05,\n",
      "       3.84216391e-05, 3.84216391e-05, 3.84216391e-05, 3.84216391e-05,\n",
      "       3.84600607e-02, 7.68816998e-02, 1.53724978e-01, 3.84216391e-05,\n",
      "       3.84216391e-05, 3.84216391e-05, 3.84600607e-02, 3.84600607e-02,\n",
      "       3.84600607e-02, 3.84600607e-02, 3.84600607e-02]), array([1.24851813e-01, 6.23947089e-05, 2.49641230e-01, 6.23947089e-05,\n",
      "       6.23947089e-05, 6.23947089e-05, 6.23947089e-05, 6.24571036e-02,\n",
      "       6.23947089e-05, 6.23947089e-05, 6.23947089e-05, 1.24851813e-01,\n",
      "       6.24571036e-02, 6.24571036e-02, 6.24571036e-02, 6.24571036e-02,\n",
      "       6.23947089e-05, 6.23947089e-05, 6.23947089e-05, 6.24571036e-02,\n",
      "       6.24571036e-02, 6.24571036e-02, 6.23947089e-05, 6.23947089e-05,\n",
      "       6.23947089e-05, 6.23947089e-05, 6.23947089e-05])]\n"
     ]
    }
   ],
   "source": [
    "phi = [(word_topic+eta)[i] / np.sum(word_topic+eta, axis=1)[i] for i in range(2)] # topic probabilities per document\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
